{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "from util import load_mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6, 6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(\"Label: %i\" % y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28, 28]), cmap='gray');\n",
    "\n",
    "from util import iterate_minibatches\n",
    "\n",
    "def train_epoch(model, optimizer, batchsize=32):\n",
    "    loss_log, acc_log = [], []\n",
    "    model.train()\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=batchsize, shuffle=True):\n",
    "        data = torch.from_numpy(x_batch.astype(np.float32))\n",
    "        target = torch.from_numpy(y_batch.astype(np.int64))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        pred = torch.max(output, 1)[1].numpy()\n",
    "        acc = np.mean(pred == y_batch)\n",
    "        acc_log.append(acc)\n",
    "        \n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log, acc_log\n",
    "\n",
    "def test(model):\n",
    "    loss_log, acc_log = [], []\n",
    "    model.eval()\n",
    "    for x_batch, y_batch in iterate_minibatches(X_val, y_val, batchsize=32, shuffle=True):\n",
    "        data = torch.from_numpy(x_batch.astype(np.float32))\n",
    "        target = torch.from_numpy(y_batch.astype(np.int64))\n",
    "\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        pred = torch.max(output, 1)[1].numpy()\n",
    "        acc = np.mean(pred == y_batch)\n",
    "        acc_log.append(acc)\n",
    "        \n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log, acc_log\n",
    "\n",
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)\n",
    "    \n",
    "    points = np.array(val_history)\n",
    "    \n",
    "    plt.scatter(points[:, 0], points[:, 1], marker='+', s=180, c='orange', label='val', zorder=2)\n",
    "    plt.xlabel('train steps')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs):\n",
    "    train_log, train_acc_log = [], []\n",
    "    val_log, val_acc_log = [], []\n",
    "\n",
    "    batchsize = 32\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {} of {}\".format(epoch, n_epochs))\n",
    "        train_loss, train_acc = train_epoch(model, opt, batchsize=batchsize)\n",
    "\n",
    "        val_loss, val_acc = test(model)\n",
    "\n",
    "        train_log.extend(train_loss)\n",
    "        train_acc_log.extend(train_acc)\n",
    "\n",
    "        steps = len(X_train) / batchsize\n",
    "        val_log.append((steps * (epoch + 1), np.mean(val_loss)))\n",
    "        val_acc_log.append((steps * (epoch + 1), np.mean(val_acc)))\n",
    "\n",
    "        clear_output()\n",
    "        plot_history(train_log, val_log)    \n",
    "        plot_history(train_acc_log, val_acc_log, title='accuracy')\n",
    "        print(\"Epoch {} error = {:.2%}\".format(epoch, 1 - val_acc_log[-1][1]))\n",
    "            \n",
    "    print(\"Final error: {:.2%}\".format(1 - val_acc_log[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem a** Implement CNN which has 2 conv layers, and max-pool, use relu activation. The last layer is fully connected. Find parameters so that output of the last conv layer is 4 x 4 x 16. \n",
    "\n",
    "We use nn.Sequential, read the documentation.\n",
    "\n",
    "You need to get the training error rate below 1.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # <your code here>\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(4 * 4 * 16, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # <your code here>\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "model = ConvNet()\n",
    "print(\"Total number of trainable parameters:\", count_parameters(model))\n",
    "\n",
    "%%time\n",
    "\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "train(model, opt, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem b:** \n",
    "* Why we use CNN for images rather than fully connected layers?\n",
    "* CNN have smalle number of parameters. Why it takes longer to train those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RNN\n",
    "We will analyze names dataset and will detect the country of origin for a given name. Our RNN will take a name and process it character-by-character. First we download the files and reshape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-25 10:47:51--  https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
      "Resolving download.pytorch.org (download.pytorch.org)... 99.84.181.24, 99.84.181.45, 99.84.181.40, ...\n",
      "Connecting to download.pytorch.org (download.pytorch.org)|99.84.181.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 47286322 (45M) [application/zip]\n",
      "Saving to: ‘hymenoptera_data.zip.2’\n",
      "\n",
      "hymenoptera_data.zi 100%[===================>]  45.10M  11.2MB/s    in 4.0s    \n",
      "\n",
      "2019-09-25 10:47:55 (11.2 MB/s) - ‘hymenoptera_data.zip.2’ saved [47286322/47286322]\n",
      "\n",
      "Archive:  ./hymenoptera_data.zip\n",
      "replace hymenoptera_data/train/ants/0013035.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "# On a windows machine, you need to download and unzip by yourself\n",
    "! wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "! unzip ./hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a few convenience functions to convert words to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem a**: define the last function for converting word to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    # <your code here>\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem b** Define one-layer RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        # <your code here>\n",
    "        # <end>\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # <your code here>\n",
    "        # <end>\n",
    "        return hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnncell = RNNCell(n_letters, n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict using liner classifier (the last layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier = nn.Sequential(nn.Linear(n_hidden, n_categories), nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything works correctly: outputs should be log-probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = letterToTensor('A')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output = classifier(rnncell(input, hidden))\n",
    "print(output)\n",
    "print(torch.exp(output).sum())\n",
    "\n",
    "input = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output = classifier(rnncell(input[0], hidden))\n",
    "print(output)\n",
    "print(torch.exp(output).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will use mini-batch SGD. Below are several helper functions to set it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem c** Implement function train. If you did everything correctly the train accuracy should be at least  70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def train(category, category_tensor, line_tensor, optimizer):\n",
    "    hidden = rnncell.initHidden()\n",
    "\n",
    "    rnncell.zero_grad()\n",
    "    classifier.zero_grad()\n",
    "\n",
    "    # <your code here>\n",
    "    # use rnncell and classifier\n",
    "    # <end>\n",
    "\n",
    "    loss = F.nll_loss(output, category_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    acc = (categoryFromOutput(output)[0] == category)\n",
    "\n",
    "    return loss.item(), acc\n",
    "\n",
    "n_iters = 50000\n",
    "plot_every = 1000\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "current_acc = 0\n",
    "all_accs = []\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "rnncell = RNNCell(n_letters, n_hidden)\n",
    "classifier = nn.Sequential(nn.Linear(n_hidden, n_categories), nn.LogSoftmax(dim=1))\n",
    "params = list(rnncell.parameters()) + list(classifier.parameters())\n",
    "opt = torch.optim.RMSprop(params, lr=0.001)\n",
    "for iter in trange(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    loss, acc = train(category, category_tensor, line_tensor, opt)\n",
    "    current_loss += loss\n",
    "    current_acc += acc\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        all_accs.append(current_acc / plot_every)\n",
    "        current_acc = 0\n",
    "        \n",
    "plt.figure()\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(all_losses)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(all_accs)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MNIST dataset, but analyze it using fully-connected networks. The goal is not to find the best architecture. We are interested in learning process for deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a flexible network init funciton that allows to change number of layers. Also you need to save gradients at each layer, so we can analyze those. \n",
    "\n",
    "**Problem a** complete the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDenseNet(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_size, activation):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        l0 = nn.Linear(X_train.shape[1], hidden_size)\n",
    "        self.weights = [l0.weight]\n",
    "        self.layers = [l0]\n",
    "        \n",
    "        # <your code here>\n",
    "        \n",
    "        self.seq = nn.Sequential(*self.layers)\n",
    "        \n",
    "        for l in self.weights:\n",
    "            l.retain_grad()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.seq(x)\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify our train function so that it plots gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse.linalg\n",
    "\n",
    "def train_epoch_grad(model, optimizer, batchsize=32):\n",
    "    loss_log, acc_log = [], []\n",
    "    grads = [[] for l in model.weights]\n",
    "    model.train()\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=batchsize, shuffle=True):\n",
    "        # data preparation\n",
    "        data = torch.from_numpy(x_batch.astype(np.float32))\n",
    "        target = torch.from_numpy(y_batch.astype(np.int64))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        pred = torch.max(output, 1)[1].numpy()\n",
    "        acc = np.mean(pred == y_batch)\n",
    "        acc_log.append(acc)\n",
    "        \n",
    "        loss = F.nll_loss(output, target)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # make a step\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "        \n",
    "        for g, l in zip(grads, model.weights):\n",
    "            g.append(np.linalg.norm(l.grad.numpy()))\n",
    "    return loss_log, acc_log, grads\n",
    "\n",
    "\n",
    "def train_grad(model, opt, n_epochs):\n",
    "    train_log, train_acc_log = [], []\n",
    "    val_log, val_acc_log = [], []\n",
    "    grads_log = None\n",
    "\n",
    "    batchsize = 32\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {} of {}\".format(epoch, n_epochs))\n",
    "        train_loss, train_acc, grads = train_epoch_grad(model, opt, batchsize=batchsize)\n",
    "        if grads_log is None:\n",
    "            grads_log = grads\n",
    "        else:\n",
    "            for a, b in zip(grads_log, grads):\n",
    "                a.extend(b)\n",
    "\n",
    "        val_loss, val_acc = test(model)\n",
    "\n",
    "        train_log.extend(train_loss)\n",
    "        train_acc_log.extend(train_acc)\n",
    "\n",
    "        steps = len(X_train) / batchsize\n",
    "        val_log.append((steps * (epoch + 1), np.mean(val_loss)))\n",
    "        val_acc_log.append((steps * (epoch + 1), np.mean(val_acc)))\n",
    "\n",
    "        # display all metrics\n",
    "        clear_output()\n",
    "        plot_history(train_log, val_log)    \n",
    "        plot_history(train_acc_log, val_acc_log, title='accuracy')    \n",
    "\n",
    "        plt.figure()\n",
    "        all_vals = []\n",
    "        for i, g in enumerate(grads_log):\n",
    "            w = np.ones(100)\n",
    "            w /= w.sum()\n",
    "            vals = np.convolve(w, g, mode='valid')\n",
    "            plt.semilogy(vals, label=str(i+1), color=plt.cm.coolwarm((i / len(grads_log))))\n",
    "            all_vals.extend(vals)\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem b**\n",
    "* Train network with more than 10 layers with sigmoid activation function. Investigate how depth affects the quality of the learning process and gradient changes on the layers that are far from the output. \n",
    "* Change activation to ReLU and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add skip-connections (like in ResNet) instead of changing sigmoid to  relu and see what happens. \n",
    "\n",
    "We cannot use nn.Sequential anymore, we need to apply our layers one-by-one. However, still add those to a nn.Sequential, otherwise torch won't be able to optimize.\n",
    "\n",
    "**Problem c** complete the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDenseResNet(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_size, activation):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        l0 = nn.Linear(X_train.shape[1], hidden_size)\n",
    "        self.weights = [l0.weight]\n",
    "        self.layers = [l0]\n",
    "        \n",
    "        for i in range(1, n_layers - 1):\n",
    "            l = nn.Linear(hidden_size, hidden_size)\n",
    "            self.layers.append(l)\n",
    "            self.weights.append(l.weight)\n",
    "            \n",
    "        l = nn.Linear(hidden_size, 10)\n",
    "        self.layers.append(l)\n",
    "        self.weights.append(l.weight)\n",
    "        \n",
    "        self.seq = nn.Sequential(*self.layers)\n",
    "        \n",
    "        for l in self.weights:\n",
    "            l.retain_grad()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # <your code here>\n",
    "        \n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that this architecture can be trained even with large number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepDenseResNet(n_layers=20, hidden_size=10, activation=nn.Sigmoid)\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "train_grad(model, opt, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
